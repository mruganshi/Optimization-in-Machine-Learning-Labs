{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02wlyoUcCMpw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cvxopt as cp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-1"
      ],
      "metadata": {
        "id": "6TncJKMcEwqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def q1(R):\n",
        "    def objective_function(x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        return (x1 - 4)**4 + (x1 - 2*x2)**2\n",
        "\n",
        "    def grad_function(x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        g = np.zeros((2, 1))\n",
        "        g[0] = 4 * (x1 - 4)**3 + 2 * (x1 - 2*x2)\n",
        "        g[1] = -4 * (x1 - 2*x2)\n",
        "        return g\n",
        "\n",
        "    x0 = np.array([[1.0], [2.0]])\n",
        "    beta_1 = 1e-4\n",
        "    beta_2 = 0.9\n",
        "    r = 0.5\n",
        "    eps = 1e-4\n",
        "    max_iterations = 500\n",
        "    iter_count = 0\n",
        "    func_evals = 0\n",
        "    grad_evals = 0\n",
        "\n",
        "    iter = 0\n",
        "    while np.linalg.norm(grad_function(x0)) > eps and iter<5000:\n",
        "\n",
        "        d0, alpha = -grad_function(x0),1\n",
        "        grad_evals+=2\n",
        "        while (objective_function(x0 + alpha*d0) > objective_function(x0) + alpha*beta_1*np.dot(grad_function(x0).T,d0)) or (np.dot(grad_function(x0 + alpha*d0).T,d0) < beta_2*np.dot(grad_function(x0).T,d0)):\n",
        "            alpha = alpha*r\n",
        "            grad_evals+=1\n",
        "            func_evals+=1\n",
        "\n",
        "        x0, iter = x0 + alpha*d0, iter + 1\n",
        "    print(\"Solution Found: \",x0)\n",
        "    print(\"Number of iteration: \",iter)\n",
        "    print(\"Function iteration: \", func_evals)\n",
        "    print(\"Grad iteration: \", grad_evals)\n",
        "    print(\"Optimal Value: \", objective_function(x0))\n",
        "\n",
        "\n",
        "\n",
        "roll_number = 14\n",
        "last_digit = roll_number % 10\n",
        "if last_digit == 0:\n",
        "    R = 1.75\n",
        "else:\n",
        "    R = last_digit\n",
        "\n",
        "q1(R)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Qwl-FNM3ds",
        "outputId": "d1742f94-c1e4-43c6-b19b-9869b2b5febf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution Found:  [[4.02807265]\n",
            " [2.01404756]]\n",
            "Number of iteration:  660\n",
            "Function iteration:  1357\n",
            "Grad iteration:  2017\n",
            "Optimal Value:  [6.21565069e-07]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-2"
      ],
      "metadata": {
        "id": "QKKlQmXOOJfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def q1(R):\n",
        "    def objective_function(x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        return (x1 - 4)**4 + (x1 - 2*x2)**2\n",
        "\n",
        "    def grad_function(x):\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        g = np.zeros((2, 1))\n",
        "        g[0] = 4 * (x1 - 4)**3 + 2 * (x1 - 2*x2)\n",
        "        g[1] = -4 * (x1 - 2*x2)\n",
        "        return g\n",
        "\n",
        "    x0 = np.array([[1], [2]])\n",
        "    beta_1 = 1e-4\n",
        "    beta_2 = 0.9\n",
        "    r = 0.5\n",
        "    eps = 1e-4\n",
        "    max_iterations = 500\n",
        "    iter_count = 0\n",
        "    func_evals = 0\n",
        "    grad_evals = 0\n",
        "    B=np.array([[8,2],[2,4]])\n",
        "    iter = 0\n",
        "    while np.linalg.norm(grad_function(x0)) > eps and iter<5000:\n",
        "        d0, alpha = B.T@(-grad_function(x0)),1\n",
        "        grad_evals+=2\n",
        "        while objective_function(x0 + alpha*d0) > objective_function(x0) + alpha*beta_1*np.dot(grad_function(x0).T,d0) or np.dot(grad_function(x0 + alpha*d0).T,d0) < beta_2*np.dot(grad_function(x0).T,d0):\n",
        "            alpha = alpha*r\n",
        "            grad_evals+=1\n",
        "            func_evals+=1\n",
        "        x0, iter = x0 + alpha*d0, iter + 1\n",
        "    print(\"Solution Found: \",x0)\n",
        "    print(\"Number of iteration: \",iter)\n",
        "    print(\"Function iteration: \", func_evals)\n",
        "    print(\"Grad iteration: \", grad_evals)\n",
        "    print(\"Optimal Value: \", objective_function(x0))\n",
        "\n",
        "\n",
        "roll_number = 14\n",
        "last_digit = roll_number % 10\n",
        "if last_digit == 0:\n",
        "    R = 1.75\n",
        "else:\n",
        "    R = last_digit\n",
        "\n",
        "q1(R)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD6VDKLZOKqN",
        "outputId": "dbbe058c-0797-406f-d2bd-ee49d37d9641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution Found:  [[3.98573775]\n",
            " [1.99286814]]\n",
            "Number of iteration:  1374\n",
            "Function iteration:  5499\n",
            "Grad iteration:  8247\n",
            "Optimal Value:  [4.13785113e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of iteration using bregman function is greater than without bregman function."
      ],
      "metadata": {
        "id": "GTrTQoTtXuvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-3"
      ],
      "metadata": {
        "id": "nyE_hf_SNfCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def F(x):\n",
        "    F1 = x[0]**2 + x[1]**2 - 2\n",
        "    F2 = np.exp(x[0] - 1) + x[1]**3 - 2\n",
        "    return np.array([F1, F2])\n",
        "\n",
        "def Jacobian(x):\n",
        "    J11 = 2*x[0]\n",
        "    J12 = 2*x[1]\n",
        "    J21 = np.exp(x[0] - 1)\n",
        "    J22 = 3*x[1]**2\n",
        "    return np.array([[J11, J12], [J21, J22]])\n",
        "\n",
        "def newton_method(x0, epsilon=1e-4, max_iterations=500):\n",
        "    x = x0\n",
        "    k = 0\n",
        "    while np.linalg.norm(F(x)) > epsilon and k < max_iterations:\n",
        "        d = np.linalg.solve(Jacobian(x), -F(x))\n",
        "        x = x + d\n",
        "        k = k + 1\n",
        "    return x, k\n",
        "\n",
        "x0 = np.array([2.4, 2.4])\n",
        "\n",
        "# Solving the system of equations using Newton's method\n",
        "solution = newton_method(x0, epsilon=1e-4, max_iterations=500)\n",
        "print(\"Solution is given as: \", solution[0])\n",
        "print(\"Number of iteration is: \",solution[1])"
      ],
      "metadata": {
        "id": "lBQ-1drbhno-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c8e1a3-44f7-42ad-c50c-4f81102a9e63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution is given as:  [1.         1.00000001]\n",
            "Number of iteration is:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-4"
      ],
      "metadata": {
        "id": "S2z9yijtPJFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#  the objective function f(x) = (x1 - r)^4 + (x1 - 2x2)^2\n",
        "def f(x, r):\n",
        "    return (x[0] - r)**4 + (x[0] - 2*x[1])**2\n",
        "\n",
        "#  the gradient of f(x) as a vector\n",
        "def grad_f(x, r):\n",
        "    return np.array([4*(x[0] - r)**3 + 2*(x[0] - 2*x[1]), -4*(x[0] - 2*x[1])])\n",
        "\n",
        "#  the Hessian matrix of f(x) as a 2x2 matrix\n",
        "def hess_f(x, r):\n",
        "    return np.array([[12*(x[0] - r)**2 + 2, -4], [-4, 8]])\n",
        "\n",
        "#  the Newton method for minimizing f(x)\n",
        "def newton_method(f, grad_f, hess_f, x0, r, tol=1e-4, max_iter=500):\n",
        "    x = x0\n",
        "    k = 0\n",
        "    while np.linalg.norm(grad_f(x, r)) > tol and k < max_iter:\n",
        "        d = np.linalg.solve(hess_f(x, r), -grad_f(x, r))\n",
        "        x = x + d\n",
        "        k = k + 1\n",
        "    return x, k\n",
        "\n",
        "r = 4\n",
        "x0 = np.array([r + 1, r - 1])\n",
        "x_min, k = newton_method(f, grad_f, hess_f, x0, r)\n",
        "\n",
        "print(\"The minimum value of f(x) is\", f(x_min, r))\n",
        "print(\"The optimal solution is x =\", x_min)\n",
        "print(\"The number of iterations is\", k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D34pgKcVPFcO",
        "outputId": "620aa4d7-a173-4c35-f5e2-79afc5cce7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of f(x) is 4.5784099211823084e-07\n",
            "The optimal solution is x = [4.02601229 2.01300615]\n",
            "The number of iterations is 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-5"
      ],
      "metadata": {
        "id": "s7_h8qFoQI-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "def f1(x):\n",
        "    n = len(x)\n",
        "    I1 = [i for i in range(2, n+1) if i % 2 == 1]\n",
        "    denominator = len(I1)\n",
        "    result = x[0]\n",
        "    for i in I1:\n",
        "        result += ((x[i-1] - np.sin(6 * np.pi * x[0] + (i * np.pi / n)))**2 )*(2/(denominator))\n",
        "    return result\n",
        "\n",
        "#function f2\n",
        "def f2(x):\n",
        "    n = len(x)\n",
        "    I2 = [i for i in range(2, n+1) if i % 2 == 0]\n",
        "    denominator = len(I2)\n",
        "    result = 1 - np.sqrt(x[0])\n",
        "    for i in I2:\n",
        "        result += ((x[i-1] - np.sin(6 * np.pi * x[0] + (i * np.pi / n)))**2 )*(2/(denominator))\n",
        "    return result\n",
        "\n",
        "def objective_function(x, r,n):\n",
        "    #n=len(x)\n",
        "    return (r*0.1) * f1(x) + (1 - r*(0.1)) * f2(x)\n",
        "\n",
        "def is_valid(x):\n",
        "    return x[0] >= 0.001 and x[0] <= 1.0 and all(x[i] >= -1.0 and x[i] <= 1.0 for i in range(1, len(x)))\n",
        "\n",
        "def grad_hess(x):\n",
        "    r=4\n",
        "    n = len(x)\n",
        "    f0 = objective_function(x, r, n)\n",
        "    h = 10**(-5)\n",
        "    b = np.zeros(n)\n",
        "    g = np.zeros(n)\n",
        "    H = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        x1 = x.copy()\n",
        "        x1[i] = x1[i] + h\n",
        "        #print(objective_function(x1, r, n))\n",
        "        b[i] = objective_function(x1, r, n)\n",
        "        g[i] = (objective_function(x1, r, n) - f0) / h\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            x2 = x.copy()\n",
        "            x2[i] = x2[i] + h\n",
        "            x2[j] = x2[j] + h\n",
        "            H[i][j] = (objective_function(x2, r, n) - b[i] - b[j] + f0) / h**2\n",
        "    return g, H\n",
        "\n",
        "def newton_method(x0, epsilon=1e-4, max_iter=500):\n",
        "    x = x0\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad, hess = grad_hess(x)\n",
        "        x_new = x - np.linalg.inv(hess) @ grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon and is_valid(x_new):\n",
        "            quad_product = grad @ hess @ grad\n",
        "            return x_new, i+1, quad_product\n",
        "        x = x_new\n",
        "    return x_new, max_iter, None\n",
        "\n",
        "x0=[]\n",
        "x0.append(random.uniform(0.001, 1))\n",
        "for i in range(29):\n",
        "  x0.append(random.uniform(-1, 1))\n",
        "solution = newton_method(x0, epsilon=1e-4, max_iter=500)\n",
        "solution\n",
        "print(\"Solution Found: \",solution[0])\n",
        "print(\"NUmber of iteration: \",solution[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EogIdnxbPwoI",
        "outputId": "b617afdd-e686-454b-88bd-c523039e67e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution Found:  [ 0.55188179 -0.92740013 -0.96142211 -0.9849106  -0.99760826 -0.99937597\n",
            " -0.99019437 -0.97016403 -0.93950444 -0.89855149 -0.84775388 -0.78766815\n",
            " -0.71895263 -0.64236016 -0.55872991 -0.46897816 -0.37408824 -0.27509978\n",
            " -0.17309733 -0.06919844  0.03545854  0.13972698  0.24246449  0.34254545\n",
            "  0.43887335  0.53039282  0.61610113  0.69505925  0.76640211  0.82934805]\n",
            "NUmber of iteration:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-6"
      ],
      "metadata": {
        "id": "plDvmK3tnA6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "# Logistic Regression Cost Function\n",
        "def cost_function(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = expit(X @ theta)\n",
        "    epsilon = 1e-5\n",
        "    cost = -1/m * (y.T @ np.log(h + epsilon) + (1 - y).T @ np.log(1 - h + epsilon))\n",
        "    return cost\n",
        "\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = expit(X @ theta)\n",
        "        gradient = 1/m * X.T @ (h - y)\n",
        "        theta = theta - alpha * gradient\n",
        "        J_history.append(cost_function(theta, X, y))\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "# Mirror Descent\n",
        "def mirror_descent(X, y, theta, alpha, num_iters):\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "\n",
        "    # Symmetric positive definite matrix\n",
        "    B = np.random.uniform(low=0.0, high=1.0, size=(X.shape[1], X.shape[1]))\n",
        "    np.fill_diagonal(B, np.random.uniform(low=5.0, high=10.0))\n",
        "\n",
        "    # Ensure B is symmetric\n",
        "    B = (B + B.T) / 2\n",
        "\n",
        "    # Ensure B is positive definite\n",
        "    min_eig = np.min(np.real(np.linalg.eigvals(B)))\n",
        "    if min_eig < 0:\n",
        "        B -= 10*min_eig * np.eye(*B.shape)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = expit(X @ theta)\n",
        "        gradient = 1/m * X.T @ (h - y)\n",
        "        theta = theta - alpha * B @ gradient\n",
        "        J_history.append(cost_function(theta, X, y))\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "\n",
        "# Newton's Method\n",
        "def newton_method(X, y, theta, num_iters):\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = expit(X @ theta)\n",
        "        gradient = 1/m * X.T @ (h - y)\n",
        "        Hessian = 1/m * X.T @ np.diag(h) @ np.diag(1-h) @ X\n",
        "        theta = theta - np.linalg.inv(Hessian) @ gradient\n",
        "        J_history.append(cost_function(theta, X, y))\n",
        "\n",
        "    return theta, J_history\n"
      ],
      "metadata": {
        "id": "-_KxnzXLQ1BK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('diabetes2.csv')\n",
        "data=data.values\n",
        "x=data[:,:-1]\n",
        "y=data[:,-1]\n",
        "len(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TgIM2CSxVd",
        "outputId": "f8365b84-b7b2-4b36-c127-85d4489966c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.01  # learning rate\n",
        "num_iters = 1000  # number of iterations\n",
        "theta = np.zeros(x.shape[1])\n",
        "theta_gd, J_history_gd = gradient_descent(x, y, theta, alpha, num_iters)\n",
        "print('Gradient Descent Theta:', theta_gd)\n",
        "print('Gradient Descent Final Cost:', J_history_gd[-1])\n",
        "\n",
        "# Solve logistic regression using mirror descent\n",
        "theta_md, J_history_md = mirror_descent(x, y, theta, alpha, num_iters)\n",
        "print('Mirror Descent Theta:', theta_md)\n",
        "print('Mirror Descent Final Cost:', J_history_md[-1])\n",
        "\n",
        "# Solve logistic regression using Newton's method\n",
        "theta_nm, J_history_nm = newton_method(x, y, theta, num_iters)\n",
        "print('Newton Method Theta:', theta_nm)\n",
        "print('Newton Method Final Cost:', J_history_nm[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqnNXXwTmTj",
        "outputId": "76ac8c07-13e5-4302-edc2-e36819d67589"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent Theta: [ 1.44593357  0.01726563 -0.9973197  -0.08703344 -0.28950063  0.08390357\n",
            "  0.07322348 -0.06251354]\n",
            "Gradient Descent Final Cost: 3.9223813555307627\n",
            "Mirror Descent Theta: [ 8.9707627   3.19789972 -6.07194713 -0.04068956  0.34823517  1.58490202\n",
            "  0.45960245  0.77948102]\n",
            "Mirror Descent Final Cost: 5.2621428474204635\n",
            "Newton Method Theta: [ 1.28418054e-01  1.29358347e-02 -3.03255466e-02  1.95674545e-04\n",
            "  7.38903841e-04 -4.81362159e-03  3.20283775e-01 -1.56346467e-02]\n",
            "Newton Method Final Cost: 0.6084775937681663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgpYHf8oUTzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}